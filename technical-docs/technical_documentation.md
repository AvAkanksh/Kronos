
# Kronos: Technical Documentation

## 1. Introduction

Kronos is a foundation model specifically designed for financial time-series forecasting, particularly K-line (candlestick) data. It employs a two-stage architecture: a specialized tokenizer that converts continuous financial data into discrete tokens, and an autoregressive Transformer model that learns the "language" of these tokens to predict future sequences.

This document provides a detailed technical explanation of the Kronos architecture, its components, the training process, and how to use it for prediction.

## 2. Overall Architecture

The core of Kronos is a two-stage framework:

1.  **Tokenizer (`KronosTokenizer`)**: This component takes raw, continuous, multi-dimensional K-line data (Open, High, Low, Close, Volume, etc.) and quantizes it into hierarchical discrete tokens. This is a crucial step that transforms the noisy, continuous financial data into a format that a language model can effectively learn from.

2.  **Predictor (`Kronos`)**: This is a large, decoder-only autoregressive Transformer model. It is pre-trained on the discrete tokens generated by the tokenizer. By learning the patterns and relationships between these tokens, it can predict future token sequences, which can then be de-tokenized back into financial data.

<p align="center">
    <img src="../figures/overview.png" alt="Kronos Overview" align="center" width="700px" />
</p>

The workflow is as follows:
1.  **Training the Tokenizer**: The `KronosTokenizer` is trained to reconstruct the original financial data after quantizing it. This ensures that the tokenization process is reversible and captures the essential information from the input data.
2.  **Training the Predictor**: The trained tokenizer is used to convert a large dataset of financial time series into token sequences. The `Kronos` model is then trained on these sequences to predict the next token in a sequence, given the previous tokens.
3.  **Inference/Prediction**: To make a forecast, the historical data is first tokenized. The `Kronos` model then autoregressively predicts the future tokens. Finally, these predicted tokens are decoded back into numerical financial data.

## 3. Core Components: A Deep Dive

This section explores the key Python modules and classes that implement the Kronos architecture.

### 3.1. `model/kronos.py`

This file contains the main user-facing classes: `KronosTokenizer`, `Kronos`, and `KronosPredictor`.

#### `KronosTokenizer`

The `KronosTokenizer` is responsible for the quantization and de-quantization of financial data. It's not a simple vocabulary lookup; it's a neural network itself, composed of an encoder, a quantizer, and a decoder.

-   **Architecture**:
    -   An `embed` layer to project the input data into the model's dimension.
    -   A series of `TransformerBlock`s forming an **encoder** to process the time-series data.
    -   A `quant_embed` layer to project the encoder's output to the codebook dimension.
    -   The `BSQuantizer` (from `model/module.py`) which performs the actual quantization.
    -   `post_quant_embed` layers to project the quantized vectors back into the model's dimension.
    -   A series of `TransformerBlock`s forming a **decoder** to reconstruct the data from the quantized representation.
    -   A `head` layer to project the decoder's output back to the original data dimension.

-   **Key Methods**:
    -   `forward(x)`: The full process of encoding, quantizing, and decoding. It returns the reconstructed data, the quantization loss, and the quantized representation. It's primarily used during the tokenizer's training.
    -   `encode(x)`: Takes raw data `x` and returns the discrete token indices. This is used during the predictor's training and for inference.
    -   `decode(indices)`: Takes token `indices` and returns the reconstructed numerical data. This is used at the end of the inference process.

#### `Kronos`

This is the main autoregressive language model. It's a decoder-only Transformer that operates on the discrete tokens from `KronosTokenizer`.

-   **Architecture**:
    -   `HierarchicalEmbedding`: Since the tokens are hierarchical (composed of two parts, `s1` and `s2`), this special embedding layer handles them.
    -   `TemporalEmbedding`: Adds positional information based on time features (minute, hour, day, etc.).
    -   A series of `TransformerBlock`s: The core of the language model.
    -   `DependencyAwareLayer`: A crucial component for hierarchical token prediction. It conditions the prediction of the second part of the token (`s2`) on the first part (`s1`).
    -   `DualHead`: A prediction head that produces separate logits for `s1` and `s2`.

-   **Hierarchical Prediction Process**:
    1.  The model first predicts the logits for the first part of the token (`s1_logits`).
    2.  A sample is drawn from this distribution to get the predicted `s1` token.
    3.  The `DependencyAwareLayer` uses the embedding of this predicted `s1` token to update the model's hidden state.
    4.  The `DualHead` then uses this updated state to predict the logits for the second part of the token (`s2_logits`).

-   **Key Methods**:
    -   `forward(...)`: The main forward pass, which takes `s1` and `s2` token IDs and returns the logits for the next tokens. It includes the logic for teacher forcing during training.
    -   `decode_s1(...)`: Used during inference. It predicts the `s1` logits and returns them along with the internal context.
    -   `decode_s2(...)`: Used during inference. It takes the context from `decode_s1` and the predicted `s1` token to predict the `s2` logits.

#### `KronosPredictor`

This is the high-level API for using Kronos. It wraps the `KronosTokenizer` and `Kronos` model into a single, easy-to-use class.

-   **Key Methods**:
    -   `predict(...)`: Makes a prediction for a single time series.
    -   `predict_batch(...)`: Makes predictions for a batch of time series.

-   **Workflow of `predict`**:
    1.  Takes a pandas DataFrame of historical data.
    2.  Performs normalization (mean/std) on the data.
    3.  Calls `auto_regressive_inference`.
    4.  De-normalizes the predicted data.
    5.  Returns a DataFrame with the forecast.

-   **`auto_regressive_inference`**:
    1.  The historical data `x` is tokenized using `tokenizer.encode(x)`.
    2.  The model enters a loop for `pred_len` steps. In each step:
        a.  It predicts the `s1` and `s2` logits for the next time step.
        b.  It samples from these logits to get the next predicted tokens.
        c.  The predicted tokens are appended to the sequence of input tokens.
    3.  After the loop, the full sequence of predicted tokens is decoded back into numerical data using `tokenizer.decode(...)`.

### 3.2. `model/module.py`

This file contains the building blocks for the `Kronos` and `KronosTokenizer` models.

-   **`BSQuantizer` (Binary Spherical Quantizer)**: This is the core of the tokenization process. It quantizes a continuous vector into a binary representation. The key idea is to project the input vector onto a sphere and then represent it by a binary code. It has a corresponding loss function (`bsq_loss`) that includes a "commitment loss" to encourage the encoder's output to be close to the quantized representation.

-   **`HierarchicalEmbedding`**: Takes the two-part token IDs (`s1_ids`, `s2_ids`), looks up their respective embeddings, and fuses them into a single vector.

-   **`DependencyAwareLayer`**: Implements the conditioning of `s2` on `s1`. It uses a cross-attention mechanism where the `query` is the embedding of the `s1` token, and the `key` and `value` are the hidden states of the main Transformer.

-   **`DualHead`**: A simple module with two linear layers to project the final hidden state into `s1` and `s2` logits.

-   **`TransformerBlock`, `MultiHeadAttentionWithRoPE`, `RotaryPositionalEmbedding`**: These are fairly standard Transformer components. Kronos uses Rotary Positional Embeddings (RoPE), which is a modern and effective way to inject positional information directly into the attention mechanism.

### 3.3. `finetune/`

This directory contains the scripts for training the `KronosTokenizer` and `Kronos` model.

#### `train_tokenizer.py`

This script fine-tunes the `KronosTokenizer`.

-   **Data**: It uses `QlibDataset` (from `finetune/dataset.py`) to load financial data.
-   **Training Loop**:
    1.  A batch of data is loaded.
    2.  The `model(batch_x)` is called, which performs the encode-quantize-decode process.
    3.  The loss is calculated as a combination of the reconstruction loss and the quantization loss from `BSQuantizer`. The reconstruction loss is the Mean Squared Error (MSE) between the original data and the reconstructed data.
        ```python
        recon_loss = F.mse_loss(z_pre, batch_x) + F.mse_loss(z, batch_x)
        loss = (recon_loss + bsq_loss) / 2
        ```
    4.  The loss is backpropagated, and the optimizer takes a step.
-   **Distributed Training**: The script is set up for Distributed Data Parallel (DDP) training using `torchrun`.

#### `train_predictor.py`

This script fine-tunes the `Kronos` language model.

-   **Prerequisites**: It requires a pre-trained `KronosTokenizer`.
-   **Data**: It also uses `QlibDataset`.
-   **Training Loop**:
    1.  A batch of data `batch_x` is loaded.
    2.  The data is tokenized on-the-fly using the frozen (non-trainable) `tokenizer.encode(batch_x)`.
    3.  The tokens are shifted to create input and target sequences (standard language model training).
    4.  The `model(...)` is called with the input tokens.
    5.  The loss is the cross-entropy loss between the predicted logits and the target tokens, calculated by the `DualHead`.
    6.  The loss is backpropagated to update the `Kronos` model's weights.

#### `dataset.py`

-   **`QlibDataset`**: This class creates sliding windows from the raw financial data. For each symbol, it pre-computes all possible start indices for a window of size `lookback_window + predict_window + 1`. During training, it randomly samples from these pre-computed indices to create a batch. It also performs instance-level normalization on each sample.

#### `config.py`

This file centralizes all configuration parameters for the project, including data paths, training hyperparameters, model paths, and logging settings.

## 4. Usage and Examples

### `examples/prediction_example.py`

This script provides a clear, minimal example of how to use Kronos for prediction.

```python
# 1. Load Model and Tokenizer
tokenizer = KronosTokenizer.from_pretrained("NeoQuasar/Kronos-Tokenizer-base")
model = Kronos.from_pretrained("NeoQuasar/Kronos-small")

# 2. Instantiate Predictor
predictor = KronosPredictor(model, tokenizer, device="cuda:0", max_context=1000)

# 3. Prepare Data
df = pd.read_csv("./data/XSHG_5min_600977.csv")
# ... prepare x_df, x_timestamp, y_timestamp ...

# 4. Make Prediction
pred_df = predictor.predict(
    df=x_df,
    x_timestamp=x_timestamp,
    y_timestamp=y_timestamp,
    pred_len=pred_len,
    # ... other parameters ...
)

# 5. Visualize Results
plot_prediction(kline_df, pred_df)
```

### `webui/app.py`

This script provides a Flask-based web interface for interacting with the Kronos model. It allows users to:
-   Load different pre-trained Kronos models (`kronos-mini`, `kronos-small`, `kronos-base`).
-   Select a data file.
-   Set prediction parameters (lookback, prediction length, etc.).
-   Visualize the historical and predicted data in an interactive chart.
-   It saves the prediction results in the `webui/prediction_results` directory.

## 5. Conclusion

The Kronos repository presents a complete, end-to-end framework for financial foundation models. It covers data preprocessing, a novel two-stage tokenization and prediction architecture, distributed training scripts, and easy-to-use examples and a web UI. The key innovation lies in the hierarchical tokenization of noisy financial data, which allows a powerful Transformer-based language model to be applied effectively to the domain of quantitative finance.
